{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Heeralall/cloud-computing-predict/blob/main/Data_deequ_tests_student_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EV8k1rFpDDJs"
      },
      "source": [
        "# Processing Big Data - Deequ Analysis\n",
        "\n",
        "© Explore Data Science Academy\n",
        "\n",
        "## Honour Code\n",
        "I {Koben Heeralall}, confirm - by submitting this document - that the solutions in this notebook are a result of my own work and that I abide by the [EDSA honour code](https://drive.google.com/file/d/1QDCjGZJ8-FmJE3bZdIQNwnJyQKPhHZBn/view?usp=sharing).\n",
        "    Non-compliance with the honour code constitutes a material breach of contract.\n",
        "\n",
        "\n",
        "## Context\n",
        "\n",
        "Having completed manual data quality checks, it should be obvious that the process can become quite cumbersome. As the Data Engineer in the team, you have researched some tools that could potentially save the team from having to do this cumbersome work. In your research, you have come a across a tool called [Deequ](https://github.com/awslabs/deequ), which is a library for measuring the data quality of large datasets.\n",
        "\n",
        "<div align=\"center\" style=\"width: 600px; font-size: 80%; text-align: center; margin: 0 auto\">\n",
        "<img src=\"https://github.com/Explore-AI/Pictures/raw/master/data_engineering/transform/predict/DataQuality.jpg\"\n",
        "     alt=\"Data Quality\"\n",
        "     style=\"float: center; padding-bottom=0.5em\"\n",
        "     width=100%/>\n",
        "     <p><em>Figure 1. Six dimensions of data quality</em></p>\n",
        "</div>\n",
        "\n",
        "You present this tool to your manager; he is quite impressed and gives you the go-ahead to use this in your implementation. You are now required to perform some data quality tests using this automated data testing tool.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\n",
        "!conda update -n base -c defaults conda -y\n",
        "!conda create --name py37 python=3.7 -y"
      ],
      "metadata": {
        "id": "QsnGDZqGb88M",
        "outputId": "478498bb-2f28-47bc-c0fd-ee74d4963314",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✨🍰✨ Everything looks OK!\n",
            "Channels:\n",
            " - defaults\n",
            " - conda-forge\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Solving environment: / \b\b- \b\b\\ \b\bdone\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "    current version: 24.11.3\n",
            "    latest version: 25.1.1\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base -c defaults conda\n",
            "\n",
            "\n",
            "\n",
            "# All requested packages already installed.\n",
            "\n",
            "Channels:\n",
            " - conda-forge\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Solving environment: / \b\b- \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local/envs/py37\n",
            "\n",
            "  added / updated specs:\n",
            "    - python=3.7\n",
            "\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge \n",
            "  _openmp_mutex      conda-forge/linux-64::_openmp_mutex-4.5-2_gnu \n",
            "  ca-certificates    conda-forge/linux-64::ca-certificates-2025.1.31-hbcca054_0 \n",
            "  ld_impl_linux-64   conda-forge/linux-64::ld_impl_linux-64-2.43-h712a8e2_2 \n",
            "  libffi             conda-forge/linux-64::libffi-3.4.6-h2dba641_0 \n",
            "  libgcc             conda-forge/linux-64::libgcc-14.2.0-h77fa898_1 \n",
            "  libgcc-ng          conda-forge/linux-64::libgcc-ng-14.2.0-h69a702a_1 \n",
            "  libgomp            conda-forge/linux-64::libgomp-14.2.0-h77fa898_1 \n",
            "  liblzma            conda-forge/linux-64::liblzma-5.6.4-hb9d3cd8_0 \n",
            "  liblzma-devel      conda-forge/linux-64::liblzma-devel-5.6.4-hb9d3cd8_0 \n",
            "  libnsl             conda-forge/linux-64::libnsl-2.0.1-hd590300_0 \n",
            "  libsqlite          conda-forge/linux-64::libsqlite-3.48.0-hee588c1_1 \n",
            "  libstdcxx          conda-forge/linux-64::libstdcxx-14.2.0-hc0a3c3a_1 \n",
            "  libstdcxx-ng       conda-forge/linux-64::libstdcxx-ng-14.2.0-h4852527_1 \n",
            "  libzlib            conda-forge/linux-64::libzlib-1.3.1-hb9d3cd8_2 \n",
            "  ncurses            conda-forge/linux-64::ncurses-6.5-h2d0b736_3 \n",
            "  openssl            conda-forge/linux-64::openssl-3.4.1-h7b32b05_0 \n",
            "  pip                conda-forge/noarch::pip-24.0-pyhd8ed1ab_0 \n",
            "  python             conda-forge/linux-64::python-3.7.12-hf930737_100_cpython \n",
            "  readline           conda-forge/linux-64::readline-8.2-h8228510_1 \n",
            "  setuptools         conda-forge/noarch::setuptools-69.0.3-pyhd8ed1ab_0 \n",
            "  sqlite             conda-forge/linux-64::sqlite-3.48.0-h9eae976_1 \n",
            "  tk                 conda-forge/linux-64::tk-8.6.13-noxft_h4845f30_101 \n",
            "  wheel              conda-forge/noarch::wheel-0.42.0-pyhd8ed1ab_0 \n",
            "  xz                 conda-forge/linux-64::xz-5.6.4-hbcc6ac9_0 \n",
            "  xz-gpl-tools       conda-forge/linux-64::xz-gpl-tools-5.6.4-hbcc6ac9_0 \n",
            "  xz-tools           conda-forge/linux-64::xz-tools-5.6.4-hb9d3cd8_0 \n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages:\n",
            "\n",
            "Preparing transaction: - \b\b\\ \b\b| \b\bdone\n",
            "Verifying transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Executing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "#\n",
            "# To activate this environment, use\n",
            "#\n",
            "#     $ conda activate py37\n",
            "#\n",
            "# To deactivate an active environment, use\n",
            "#\n",
            "#     $ conda deactivate\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "   eval \"$(conda shell.bash hook)\"\n",
        "   conda activate py37"
      ],
      "metadata": {
        "id": "wf5i-5YxCz8z",
        "outputId": "3cbf45e4-189b-480b-88b4-47c19918dd2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Colab is essentially running on a linux machine on Google Cloud Platform.\n",
        "This means that should you want to install something in your notebook you\n",
        "would have to run headless installs as well as wget. Copying the installs\n",
        "below will ensure that you have spark and java installed in the environment\n",
        "as well as available for the notebook.\n",
        "\"\"\"\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.0.0-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "jwHS9hPmDJUg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This section helps in rendering your notebook operable and ensuring that\n",
        "your environment variables for spark are correct. Running this cell in any\n",
        "notebook allows for any unresolved spark environment to be fixed.\n",
        "\"\"\"\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop2.7\"\n",
        "os.environ[\"SPARK_VERSION\"] = \"3.0.0\""
      ],
      "metadata": {
        "id": "MCUjaIIIDJCw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "We need to locate Spark in the system. For that, we import findspark and\n",
        "use the findspark.init() method. If you want to know the location where\n",
        "Spark is installed, use findspark.find()\n",
        "\"\"\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "# findspark.find()\n",
        "# import pyspark"
      ],
      "metadata": {
        "id": "uhwMZbG0DI5e"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This cell is used to import data/files from your local machine to colab\n",
        "\"\"\"\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "z-FSZWKpI5sT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "b0883f82-1a3e-443b-897f-534b7b85b685"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9c63a4b8-f583-487f-8838-5f59b5757009\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9c63a4b8-f583-487f-8838-5f59b5757009\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving part-00000-52e1da7d-39e6-46bc-8167-46297b52948b-c000.snappy.parquet to part-00000-52e1da7d-39e6-46bc-8167-46297b52948b-c000.snappy.parquet\n",
            "Saving part-00001-52e1da7d-39e6-46bc-8167-46297b52948b-c000.snappy.parquet to part-00001-52e1da7d-39e6-46bc-8167-46297b52948b-c000.snappy.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "2fZRTgaizG9Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6b30550-b0f1-44e8-89c2-110dca823b88"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USKopMYSDDJ0"
      },
      "source": [
        "> ## 🚩️ Important Notice 🚩️\n",
        ">\n",
        ">To successfully run `pydeequ` without any errors, please make sure that you have an environment that is running pyspark version 3.0.\n",
        "> You are advised to **create a new conda environment** and install this specific version of pyspark to avoid any technical issues:\n",
        ">\n",
        "> `pip install pyspark==3.0`\n",
        "\n",
        "<br>\n",
        "\n",
        "## Import dependencies\n",
        "\n",
        "If you do not have `pydeequ` already installed, install it using the following command:\n",
        "- `pip install pydeequ`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydeequ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixoKdHqEFV3I",
        "outputId": "d64aa870-e4fa-428c-f4d8-dc758ad534fa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pydeequ in /usr/local/lib/python3.11/site-packages (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.11/site-packages (from pydeequ) (2.2.3)\n",
            "Requirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.11/site-packages (from pydeequ) (2.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/site-packages (from pandas>=0.23.0->pydeequ) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/site-packages (from pandas>=0.23.0->pydeequ) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/site-packages (from pandas>=0.23.0->pydeequ) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=0.23.0->pydeequ) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.0.0 pydeequ==1.0.1 findspark"
      ],
      "metadata": {
        "id": "H0CUnYBQD6xi",
        "outputId": "25be2535-6cda-46bf-d853-2501bb2b4f0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.0.0\n",
            "  Downloading pyspark-3.0.0.tar.gz (204.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.7/204.7 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pydeequ==1.0.1\n",
            "  Downloading pydeequ-1.0.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: findspark in /usr/local/lib/python3.11/site-packages (2.0.1)\n",
            "Collecting py4j==0.10.9 (from pyspark==3.0.0)\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.11/site-packages (from pydeequ==1.0.1) (2.2.3)\n",
            "Requirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.11/site-packages (from pydeequ==1.0.1) (2.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/site-packages (from pandas>=0.23.0->pydeequ==1.0.1) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/site-packages (from pandas>=0.23.0->pydeequ==1.0.1) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/site-packages (from pandas>=0.23.0->pydeequ==1.0.1) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=0.23.0->pydeequ==1.0.1) (1.17.0)\n",
            "Downloading pydeequ-1.0.1-py3-none-any.whl (36 kB)\n",
            "Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.0-py2.py3-none-any.whl size=205044159 sha256=be3a600bac3f4056abc0adcca594fc52186f3a1e162da0c1c0b816ff5c7c1c83\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/d5/48/7beeda5357820f04fbe868a22e1d69fb922ec1c09b815bf6ae\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark, pydeequ\n",
            "  Attempting uninstall: pydeequ\n",
            "    Found existing installation: pydeequ 1.4.0\n",
            "    Uninstalling pydeequ-1.4.0:\n",
            "      Successfully uninstalled pydeequ-1.4.0\n",
            "Successfully installed py4j-0.10.9 pydeequ-1.0.1 pyspark-3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pydeequ\n",
        "!pip install --upgrade pyspark"
      ],
      "metadata": {
        "id": "czqw8vTJLI4b",
        "outputId": "9b8b6d17-5091-47c0-db4d-5fbd16dc7222",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pydeequ in /usr/local/lib/python3.11/site-packages (1.0.1)\n",
            "Collecting pydeequ\n",
            "  Using cached pydeequ-1.4.0-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.11/site-packages (from pydeequ) (2.2.3)\n",
            "Requirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.11/site-packages (from pydeequ) (2.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/site-packages (from pandas>=0.23.0->pydeequ) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/site-packages (from pandas>=0.23.0->pydeequ) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/site-packages (from pandas>=0.23.0->pydeequ) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=0.23.0->pydeequ) (1.17.0)\n",
            "Using cached pydeequ-1.4.0-py3-none-any.whl (37 kB)\n",
            "Installing collected packages: pydeequ\n",
            "  Attempting uninstall: pydeequ\n",
            "    Found existing installation: pydeequ 1.0.1\n",
            "    Uninstalling pydeequ-1.0.1:\n",
            "      Successfully uninstalled pydeequ-1.0.1\n",
            "Successfully installed pydeequ-1.4.0\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/site-packages (3.0.0)\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.4.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.7 (from pyspark)\n",
            "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.4-py2.py3-none-any.whl size=317849766 sha256=f64dc26bacb7482399af07ec31a6395d52b070f0ee8e674d14105bb2f5ace82b\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/28/22/5dbae8a8714ef046cebd320d0ef7c92f5383903cf854c15c0c\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9\n",
            "    Uninstalling py4j-0.10.9:\n",
            "      Successfully uninstalled py4j-0.10.9\n",
            "  Attempting uninstall: pyspark\n",
            "    Found existing installation: pyspark 3.0.0\n",
            "    Uninstalling pyspark-3.0.0:\n",
            "      Successfully uninstalled pyspark-3.0.0\n",
            "Successfully installed py4j-0.10.9.7 pyspark-3.5.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "py4j"
                ]
              },
              "id": "e32a2fc231fb4c20843113c4afcbb649"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"PyDeequTest\").getOrCreate()"
      ],
      "metadata": {
        "id": "2YWJbiGcML_3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Set SPARK_VERSION environment variable\n",
        "\n",
        "os.environ[\"SPARK_VERSION\"] = \"3.1\"  # Replace with your Spark version\n",
        "\n",
        "# Now import other libraries\n",
        "\n",
        "import pyspark\n",
        "\n",
        "import pydeequ\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark session\n",
        "\n",
        "spark = SparkSession.builder.appName(\"PyDeequTest\").getOrCreate()\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "SA29yM1oMrau"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"SPARK_VERSION\"] = \"3.1\"  # Replace with your Spark version\n",
        "\n",
        "# Now you can import other libraries or run your code:\n",
        "\n",
        "import pyspark\n",
        "# ... other imports ...\n",
        "\n",
        "# Execute your script or other code:\n",
        "# exec(open('your_script.py').read())"
      ],
      "metadata": {
        "id": "wQOEhy_OMwXh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gkJ3tG1uDDJ6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pydeequ\n",
        "from pydeequ.analyzers import *\n",
        "from pydeequ.profiles import *\n",
        "from pydeequ.suggestions import *\n",
        "from pydeequ.checks import *\n",
        "from pydeequ.verification import *\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import DecimalType, DoubleType, IntegerType, DateType, NumericType, StructType, StringType, StructField"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfYDaj6cDDJ9"
      },
      "source": [
        "## Read data into spark dataframe\n",
        "\n",
        "In this notebook, we set out to run some data quality tests, with the possiblity of running end to end on the years 1963, 1974, 1985, 1996, 2007, and 2018.\n",
        "\n",
        "> ℹ️ **Instructions** ℹ️\n",
        ">\n",
        ">1. Make use of the `Data_ingestion_student_version.ipynb` notebook to create the parquet files for the following years:\n",
        ">       - 1963\n",
        ">       - 1974\n",
        ">       - 1985\n",
        ">       - 1996\n",
        ">       - 2007\n",
        ">       - 2018\n",
        ">\n",
        ">2. Ingest the data for the for the years given above. You should only do it one year at a time.\n",
        ">3. Ingest the metadata file.\n",
        "\n",
        "\n",
        "When developing your code, it will be sufficient to focus on a single year. However, after your development is done, you will need to run this notebook for all of the given years above so that you can answer all the questions given in the Data Testing MCQ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ao6fv5TDDJ-"
      },
      "outputs": [],
      "source": [
        "#TODO: Write your code here\n",
        "# Use this variable (year) to determine which year your are focusing on\n",
        "year = 1963\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi0etV9NDDKC"
      },
      "source": [
        "## **Run tests on the dataset**\n",
        "\n",
        "## Test 1 - Null values ⛔️\n",
        "For the first test, you are required to check the data for completeness.\n",
        "\n",
        "> ℹ️ **Instructions** ℹ️\n",
        ">\n",
        ">1. Make use of the `Verification Suite` and write code to check for missing values in the data.\n",
        ">2. Display the results of your test.\n",
        ">\n",
        "> *You may use as many cells as necessary*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKtagFU3DDKE"
      },
      "outputs": [],
      "source": [
        "#TODO: Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Low_wGUrDDKF"
      },
      "source": [
        "## Test 2 - Zero Values 🅾️\n",
        "\n",
        "For the second test, you are required to check for zero values within the dataset.\n",
        "\n",
        "> ℹ️ **Instructions** ℹ️\n",
        ">\n",
        ">1. Make use of the `Verification Suite` and write code to check for zero values within the data.\n",
        ">2. Display the results of your test.\n",
        ">\n",
        "> *You may use as many cells as necessary*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETmxIDkSDDKG"
      },
      "outputs": [],
      "source": [
        "#TODO: Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bThOkXQ2DDKH"
      },
      "source": [
        "## Test 3 - Negative values ➖️\n",
        "The third test requires you to check that all values in the data are positive.\n",
        "\n",
        "> ℹ️ **Instructions** ℹ️\n",
        ">\n",
        ">1. Make use of the `Verification Suite` and write code to check negative values within the dataset.\n",
        ">2. Display the results of your test.\n",
        ">\n",
        "> *You may use as many cells as necessary*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCbivSPkDDKI"
      },
      "outputs": [],
      "source": [
        "#TODO: Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LFgEe1yDDKJ"
      },
      "source": [
        "## Test 4 - Determine Maximum Values ⚠️\n",
        "\n",
        "For the fourth test, we want to find the maximum values in the dataset for the numerical fields. Extremum values can often be used to define an upper bound for the column values so we can define them as the threshold values.\n",
        "\n",
        "> ℹ️ **Instructions** ℹ️\n",
        ">\n",
        ">1. Make use of the `Column Profiler Runner` to generate summary statistics for all the available columns.\n",
        ">2. Extract the maximum values for all the numeric columns in the data.\n",
        ">\n",
        "> *You may use as many cells as necessary*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMMhyLiJDDKJ"
      },
      "outputs": [],
      "source": [
        "#TODO: Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C70lyS07DDKK"
      },
      "source": [
        "## Test 5 - Stock Tickers 💹️\n",
        "\n",
        "For the fifth test, we want to determine if the stock tickers contained in our dataset are consistent. To do this, you will need to make use of use of the metadata file to check that the stock names used in the dataframe are valid.\n",
        "\n",
        "> ℹ️ **Instructions** ℹ️\n",
        ">\n",
        ">1. Make use of the `Verification Suite` and write code to determine if the stock tickers contained in the dataset appear in the metadata file.\n",
        ">2. Display the results of your test.\n",
        ">\n",
        "> *You may use as many cells as necessary*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIxPTlYFDDKL"
      },
      "outputs": [],
      "source": [
        "#TODO: Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w76O1xRQDDKM"
      },
      "source": [
        "## Test 6 - Duplication 👥️\n",
        "Lastly, we want to determine the uniqueness of the items found in the dataframe. You need to make use of the Verification Suite to check for the validity of the stock tickers.\n",
        "\n",
        "Similar to the previous notebook - `Data_profiling_student_version.ipynb`, the first thing to check will be if the primary key values within the dataset are unique - in our case, that will be a combination of the stock name and the date. Secondly, we want to check if the entries are all unique, which is done by checking for duplicates across that whole dataset.\n",
        "\n",
        "> ℹ️ **Instructions** ℹ️\n",
        ">\n",
        ">1. Make use of the `Verification Suite` and write code to determine the uniqueness of entries contained within the dataset.\n",
        ">2. Display the results of your test.\n",
        ">\n",
        "> *You may use as many cells as necessary*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64PGs6w_DDKM"
      },
      "outputs": [],
      "source": [
        "#TODO: Write your code here"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "0b41f59b882618484a4d28c089dca4efdf4ffb1e043e654ec6730d7439b802f5"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}