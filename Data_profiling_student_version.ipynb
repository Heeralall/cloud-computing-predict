{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Heeralall/cloud-computing-predict/blob/main/Data_profiling_student_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFNACB81BX9t"
      },
      "source": [
        "# Processing Big Data - Data profiling\n",
        "\n",
        "Â© Explore Data Science Academy\n",
        "\n",
        "## Honour Code\n",
        "I {Koben Heeralall}, confirm - by submitting this document - that the solutions in this notebook are a result of my own work and that I abide by the [EDSA honour code](https://drive.google.com/file/d/1QDCjGZJ8-FmJE3bZdIQNwnJyQKPhHZBn/view?usp=sharing).\n",
        "    Non-compliance with the honour code constitutes a material breach of contract.\n",
        "\n",
        "## Context\n",
        "\n",
        "Having completed the first step - data ingestion, the data now needs to be thoroughly prepared so that it is readable, reliable and robust. As the Data Engineer in the team, this will be your responsibility. The Data Scientists are looking to you to clean this data so that model development and deployment become seamless when the data is used in a production environment. Having completed your Data Engineering course recently, your manager Gnissecorp Atadgid, asks you to create data summaries and perform checks using the six dimensions of data quality.\n",
        "\n",
        "<div align=\"center\" style=\"width: 600px; font-size: 80%; text-align: center; margin: 0 auto\">\n",
        "<img src=\"https://github.com/Explore-AI/Pictures/raw/master/data_engineering/transform/predict/DataQuality.jpg\"\n",
        "     alt=\"Data Quality\"\n",
        "     style=\"float: center; padding-bottom=0.5em\"\n",
        "     width=100%/>\n",
        "     <p><em>Figure 1. Six dimensions of data quality</em></p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CTBgtnsX_sJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-IsOo8xB7P7"
      },
      "source": [
        "### Upload from local\n",
        "The following cell is to upload a file from your local machine to Google Colab. You will have a dialogue box as indicated where you will select the file and have it uploaded to the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "lvT_lciECPs8",
        "outputId": "3c15cb52-0548-48a5-8f15-b6de2017fb7e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a7f09f34-d132-426b-acf7-8effb94c57cd\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a7f09f34-d132-426b-acf7-8effb94c57cd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "\"\"\"\n",
        "This cell is used to import data/files from your local machine to colab\n",
        "\"\"\"\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "VOoAUm_irqYH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaf5ce01-a7c7-41ca-abec-cb5c695daefc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Szv6qk0wCdLm"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Colab is essentially running on a linux machine on Google Cloud Platform.\n",
        "This means that should you want to install something in your notebook you\n",
        "would have to run headless installs as well as wget. Copying the installs\n",
        "below will ensure that you have spark and java installed in the environment\n",
        "as well as available for the notebook.\n",
        "\"\"\"\n",
        "!apt-get install openjdk-17-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.5.4/spark-3.5.4-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.4-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AQ0meY6PCch-"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This section helps in rendering your notebook operable and ensuring that\n",
        "your environment variables for spark are correct. Running this cell in any\n",
        "notebook allows for any unresolved spark environment to be fixed.\n",
        "\"\"\"\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.4-bin-hadoop3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mdqBDSWxCccH"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "We need to locate Spark in the system. For that, we import findspark and\n",
        "use the findspark.init() method. If you want to know the location where\n",
        "Spark is installed, use findspark.find()\n",
        "\"\"\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "import pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttW56rEsBX91"
      },
      "source": [
        "## Import libraries\n",
        "Below we import the libraries required to complete this section of the predict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cBDN1g8OBX92"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEuecDBjBX94"
      },
      "source": [
        "Remember that we need a `SparkContext` and `SparkSession` to interface with Spark.\n",
        "We will mostly be using the `SparkContext` to interact with RDDs\n",
        "and the `SparkSession` to interface with Python objects.\n",
        "\n",
        "> â¹ï¸ **Instructions** â¹ï¸\n",
        ">\n",
        ">Initialise a new **Spark Context** and **Session** that you will use to interface with Spark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ghkq51AcBX96"
      },
      "outputs": [],
      "source": [
        "#TODO: Write your code here.\n",
        "sc = SparkContext()\n",
        "spark = SparkSession(sc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2s9k2V3dBX97"
      },
      "source": [
        "## Parquet files\n",
        "In the previous section of the predict, you generated parquet files to your local directory. You will be making use of these files to continue with this section of the predict. Please make sure that your parquet files are specifically for the year **1962**. Any other year used outside of **1962** will produce incorrect answers and have a negative impact on your overall predict mark.\n",
        "\n",
        "> â¹ï¸ **Instructions** â¹ï¸\n",
        ">\n",
        "> Read the parquet files stored in your directory for the year **1962** into a Spark DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "L_nKNDXEBX98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "b926e951-1248-41e9-a90f-7cb238d777d8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/path/to/your/parquet/files/1962*.parquet.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-9b365f289c0a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/path/to/your/parquet/files/1962*.parquet\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    542\u001b[0m         )\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m     def text(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/path/to/your/parquet/files/1962*.parquet."
          ]
        }
      ],
      "source": [
        "#TODO: Write your code here\n",
        "from pyspark.sql import SparkSession\n",
        "path = \"/path/to/your/parquet/files/1962*.parquet\"\n",
        "df = spark.read.parquet(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DN8AxyrBX9_"
      },
      "source": [
        "## Metadata\n",
        "\n",
        "Metadata is data containing additional information about the data itself. In the cloud storage, there is a metadata file called [`symbols_valid_meta.csv`](https://processing-big-data-predict-stocks-data.s3.eu-west-1.amazonaws.com/symbols_valid_meta.csv) that is collocated with the stock market data. You will need to download this to use when performing your data quality checks.\n",
        "\n",
        "> â¹ï¸ **Instructions** â¹ï¸\n",
        ">\n",
        "> Download the metadata from the S3 bucket and read it into a Spark DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2KmMlbM5CLFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "gVYA9stsBX-A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "109ec636-9ad2-4646-edfe-b02aa4ea1199"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnsupportedOperationException",
          "evalue": "None",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnsupportedOperationException\u001b[0m             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-4c3e1dadc8f3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#TODO: Write your code here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fprocessing-big-data-predict-stocks-data.s3.eu-west-1.amazonaws.com%2Fsymbols_valid_meta.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnsupportedOperationException\u001b[0m: None"
          ]
        }
      ],
      "source": [
        "#TODO: Write your code here\n",
        "metadata = spark.read.csv('https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fprocessing-big-data-predict-stocks-data.s3.eu-west-1.amazonaws.com%2Fsymbols_valid_meta.csv',header=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgVZEqSNBX-C"
      },
      "source": [
        "## Data Accuracy\n",
        "Data accuracy is the degree to which data correctly describes a \"real world\" object or event.\n",
        "\n",
        "It is important to do checks to determine the basic integrity of the dataset; do the values fall within expected ranges?\n",
        "\n",
        "Most of the possible errors relating to data accuracy can occur at collection time. In our case, it is not possible to test the collection time accuracy, so we have to infer from ranges and summary statistics. Here you need to look closely at each field to see if its values make sense, with no strange surprises.\n",
        "\n",
        "In assessing accuracy, it is important to look into precision as well. Do you need seven decimals, or will one or two suffice?\n",
        "\n",
        "- **Measured by**: The degree to which the data mirrors the characteristics of the real-world object or objects it represents;\n",
        "- **Units**: The percentage of data entries that pass the data accuracy rules;\n",
        "- **Related to**: Validity, Uniqueness, Consistency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xNCCTp0BX-C"
      },
      "source": [
        "> â¹ï¸ **Instructions** â¹ï¸\n",
        ">\n",
        "> Generate summary statistics to explore your data. Make sure you understand the ranges, means, extremums, and deviations found in the data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKem5xm5BX-D"
      },
      "outputs": [],
      "source": [
        "#TODO: Write your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5Mwln-uBX-D"
      },
      "source": [
        "> â¹ï¸ **Instructions** â¹ï¸\n",
        ">\n",
        "> Generate histograms for the six numerical attributes found in the data to understand the distribution of values.\n",
        ">\n",
        ">*You may use as many coding cells as necessary.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N34VBTdTBX-E"
      },
      "outputs": [],
      "source": [
        "#TODO: Write your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJNUY54_BX-F"
      },
      "source": [
        "> â¹ï¸ **Instructions** â¹ï¸\n",
        ">\n",
        "> Investigate the **open** column to identify stocks that have open values greater than 2, and note any anomalies that you find in the data.\n",
        ">\n",
        ">*You may use as many coding cells as necessary.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVlMdgQ3BX-F"
      },
      "outputs": [],
      "source": [
        "#TODO: Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHyQgjCPBX-G"
      },
      "source": [
        " ## ðï¸**Notes** ðï¸\n",
        "\n",
        " *Use this cell to note down any potential findings.*\n",
        "\n",
        " 1. duplicated open values on consecutive days despite non-zero volume\n",
        " 2. \"PG\" ticker has multiple opening prices at 20.0 during 1962-08\n",
        " 3. possible loss of data quality for \"IBM\" ticker due to low number of significant figures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5RlGugzBX-G"
      },
      "source": [
        "> â¹ï¸ **Instructions** â¹ï¸\n",
        ">\n",
        "> Investigate **high**, **low**, **close**, and **adj_close** to determine if any stocks may be deviating from the normal ranges of the data set. Note down the stock(s) that you come across.\n",
        ">\n",
        ">*You may use as many coding cells as necessary.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2gTLjFCBX-H"
      },
      "outputs": [],
      "source": [
        "#TODO: Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mE57FgafBX-H"
      },
      "source": [
        " ## ðï¸**Notes** ðï¸\n",
        "\n",
        " *Use this cell to note down any potential findings.*\n",
        "\n",
        " 1. all values of \"NAV\" lie at least 3.8 stddev above the mean\n",
        " 2. stddev > mean (~3.2 times larger)\n",
        " 3. multiple repeated values along columns for all tickers\n",
        " 4. multiple 'null' values for \"low\" value of NAV"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ub2_riB3IlqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0NQNCFFBX-I"
      },
      "source": [
        "\n",
        "\n",
        "## Completeness\n",
        "\n",
        "Completeness is the proportion of stored data against the potential of â100% complete\". This is the degree to which the required data is in the dataset.\n",
        "\n",
        "Does the dataset have missing values, or if it is time-series data, does it have time period gaps? Has a bias been introduced that may change your assumptions or affect your results?\n",
        "\n",
        "Completeness issues can occur at the row level (gaps within the dataset) or the field level (one entry missing). At the field level, entire fields can being empty, or >80% of a field's data missing.\n",
        "\n",
        "Another issue that may occur is default values. A typical example of this is where a logger sends back a 0 instead of a null value, which can greatly skew any attempts at modelling. This is where it is instrumental to employ domain knowledge when assessing a dataset.\n",
        "\n",
        "- **Measured by**: A measure of the absence of blank (null) values or the presence of non-blank values;\n",
        "- **Units**: Percentage;\n",
        "- **Related to**: Validity and Accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxbeZ0gkBX-I"
      },
      "source": [
        "### Missing values\n",
        "\n",
        "> â¹ï¸ **Instructions** â¹ï¸\n",
        ">\n",
        "> Write code to identify and count the number of missing values (nulls) in the dataset. Include a percentage to describe the proportion of missing values per column. Output the results in the following manner:\n",
        ">\n",
        "> `There are <number_of_missing_values> (<percentage>) null values in <column_name> column`\n",
        ">\n",
        ">*You may use as many coding cells as necessary.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUyzUb4uBX-I"
      },
      "outputs": [],
      "source": [
        "#TODO: Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khe6EvQ3BX-J"
      },
      "source": [
        "### Columns with missing values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKI4CsKdBX-J"
      },
      "source": [
        "\n",
        "> â¹ï¸ **Instructions** â¹ï¸\n",
        ">\n",
        "> From the above result, probe the columns that are affected by the missing data to find out which stocks were affected.\n",
        ">\n",
        ">*You may use as many coding cells as necessary.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKNksmyeBX-J"
      },
      "outputs": [],
      "source": [
        "#TODO: Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnbkoZkkBX-K"
      },
      "source": [
        "#### Fix Completeness\n",
        "\n",
        "How do we deal with incomplete data?\n",
        "- Dropping missing values\n",
        "- Discard the incomplete column\n",
        "- Discard the rows containing missing data\n",
        "- Case deletion\n",
        "\n",
        "> â¹ï¸ **Instructions** â¹ï¸\n",
        ">\n",
        "> Use the appropriate strategy to remedy the missing data.\n",
        ">\n",
        ">*You may use as many coding cells as necessary.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIEltxS8BX-K",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#TODO: Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rU2gN3VBX-L"
      },
      "source": [
        "### Zero Values\n",
        "\n",
        "Take a deeper look into the entries with many zero values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXd2q5R6BX-L"
      },
      "source": [
        "#### Fix Completeness\n",
        "\n",
        "Completeness issues can be fixed through imputation of the missing data through:\n",
        "- imputation by mean/mode/median;\n",
        "- regression; or\n",
        "- KNN.\n",
        "\n",
        "> â¹ï¸ **Instructions** â¹ï¸\n",
        ">\n",
        "> Write code to identify and count the number of zeros (0) in the dataset. Include a percentage to describe the proportion of missing values per column. Output the results in the following manner:\n",
        ">\n",
        "> `There are <number_of_zeros> (<percentage>) zero values in <column_name> column`\n",
        ">\n",
        ">*You may use as many coding cells as necessary.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hj2OTyYkBX-L"
      },
      "outputs": [],
      "source": [
        "#TODO: Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOXYKUTqBX-M"
      },
      "source": [
        "From the above section, you find that there are a few columns that contain zero values. However, some of these are true zeros and are explainable. Your task is to distinguish which column should undergo data imputation.\n",
        "\n",
        "> â¹ï¸ **Instructions** â¹ï¸\n",
        ">\n",
        "> Investigate the columns with zero values and determine which one should undergo data imputation. Take note of the stock and month on which zero values occurred.\n",
        ">\n",
        ">*You may use as many coding cells as necessary.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac6N5cRQBX-M"
      },
      "outputs": [],
      "source": [
        "#TODO: Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYhwPvLRBX-N"
      },
      "source": [
        "> â¹ï¸ **Instructions** â¹ï¸\n",
        ">\n",
        "> Once you have identified the column that needs to undergo imputation, update the values for the affected records by using the average value for the affected stock.\n",
        ">\n",
        ">*You may use as many coding cells as necessary.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVdvHqQTBX-O"
      },
      "outputs": [],
      "source": [
        "#TODO: Write your code here\n",
        "#from pyspark.sql.functions import avg, when"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fjXIh5xBX-P"
      },
      "source": [
        "\n",
        "## Consistency\n",
        "\n",
        "Consistency is the absence of difference when comparing two or more representations of a thing against a reference.\n",
        "\n",
        "Data entries that refer to the same record or entity have to be consistent across all entries, e.g., if you are dealing with records from a logger in the field, the entries for that logger have to remain consistent, and the name or primary key of that logger cannot change from one entry to another.\n",
        "\n",
        "For example, 'Logger1', 'Loger1' and 'Logge1' are examples of inconsistent keys.\n",
        "\n",
        "This is not just within a single table but also becomes more important if you are dealing with relational data. In which case, the mappings between tables and systems must be consistent. If not, the relationships will be completely lost between the tables and referential integrity compromised.\n",
        "\n",
        "- **Measured by**: Analysis of pattern and/or value frequency;\n",
        "- **Units**: Percentage;\n",
        "- **Related to**: Accuracy, Validity, and Uniqueness."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnL-Qyy1BX-Q"
      },
      "source": [
        "> â¹ï¸ **Instructions** â¹ï¸\n",
        ">\n",
        "> There currently exists a stock that has inconsistent naming. Make use of the metadata to determine which stock is inconsistently named, then update the dataframe appropriately to get rid of this inconsistency.\n",
        ">\n",
        ">*You may use as many coding cells as necessary.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzNvo6W_BX-Q"
      },
      "outputs": [],
      "source": [
        "#TODO: Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vh831hmrBX-R"
      },
      "source": [
        "## Timeliness\n",
        "\n",
        "Timeliness is the degree to which data represent reality from the required point in time.\n",
        "\n",
        "Timeliness expects that the data within your dataset is sufficiently up to date. If you are trying to answer questions that relate to recent problems, having timely data is extremely important. For example, you cannot use current flight patterns to model how many aeroplanes will be required by a large aeronautics company within the next 5-10 years.\n",
        "\n",
        "Similarly, when answering questions that require real-time answers (e.g., predicting when a pipe will burst in a manufacturing plant), you have to be set up to receive real-time data from sensors and loggers.\n",
        "\n",
        "- **Measured by**: Time difference;\n",
        "- **Units**: Time;\n",
        "- **Related to**: Accuracy because it will decay as time progress."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlRTEAHvBX-S"
      },
      "source": [
        "It is important to see the latest value for each of the stocks that we are looking at:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ROrgcV5BX-S"
      },
      "outputs": [],
      "source": [
        "df.groupBy('stock').agg(F.max('Date')).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oe13UFxBX-S"
      },
      "source": [
        "Sometimes, some of these axes of data quality will be less important than others.\n",
        "This is one of those cases where it is less important to have timely data, since\n",
        "we are trying to create a training dataset for a stock market prediction algorithm.\n",
        "\n",
        "It is important to know the context in which you are doing your modelling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnT1-VoCBX-S"
      },
      "source": [
        "### Gaps in the dataset\n",
        "\n",
        "Let's see if we can find inconsistencies in the time series by having a look at the number of entries for each of the tickers.\n",
        "\n",
        "> â¹ï¸ **Instructions** â¹ï¸\n",
        ">\n",
        "> Uncomment and use the below code to determine which dates had entries that were not equal to 20. You may have to change the name of the dataframe to see the resultant output\n",
        ">\n",
        ">*You may use as many coding cells as necessary.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8NmKjR1BX-S"
      },
      "outputs": [],
      "source": [
        "#df.orderBy('date').groupby('date').count().where(F.col('count') != 20).show(400)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5yDRErZBX-T"
      },
      "source": [
        "> â¹ï¸ **Instructions** â¹ï¸\n",
        ">\n",
        "> From the above result, investigate the number of times a stock appears for the given month. You can infer the months by using the output of the previous cell.\n",
        ">\n",
        ">*You may use as many coding cells as necessary.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwIc4sKvBX-T"
      },
      "outputs": [],
      "source": [
        "#TODO: Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UloHBfvBX-U"
      },
      "source": [
        "## Uniqueness\n",
        "\n",
        "Uniqueness requires that nothing will be recorded more than once based upon how that thing is identified. It is the inverse of an assessment of the level of duplication.\n",
        "\n",
        "Each entry within the dataset should only relate to a single event that has occurred and thus should not be duplicated. This is largely mediated by having the appropriate primary key, which means sticking to the requirements of a good primary key. All fields in the tables should be non-transitively dependent on the primary key.\n",
        "\n",
        "As such, deduplication of the dataset may be required.\n",
        "\n",
        "- **Measured by**: Analysis of the number of things assessed in the âreal worldâ compared to the number of records of things in the dataset. This requires a reference dataset which is the ground truth;\n",
        "- **Units**: Percentage;\n",
        "- **Related to**: Consistency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm2Hc622BX-U"
      },
      "source": [
        "### Duplication Test\n",
        "For time-series data, it is important to check for duplications, as we typically expect all values to be unique within the dataset.\n",
        "\n",
        "The first thing to check will be if the primary key values within the dataset are unique - in our case, that will be a combination of the stock name and the date.\n",
        "\n",
        "Secondly, we want to check if the entries are all unique, which is done by checking for duplicates across that whole dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5Fv8MBeBX-U"
      },
      "source": [
        "> â¹ï¸ **Instructions** â¹ï¸\n",
        ">\n",
        "> Write code to determine if there are any duplicates within the data, and then proceed to correct this by dropping them from the dataframe.\n",
        ">\n",
        ">*You may use as many coding cells as necessary.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UoIpDBnBX-V"
      },
      "outputs": [],
      "source": [
        "#TODO: Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3eR4G3JBX-V"
      },
      "source": [
        "## Validity\n",
        "Data is valid if it conforms to the syntax (format, type, range) of its definition.\n",
        "\n",
        "Certain values within a field may have specific criteria required to make it valid, e.g., numerical columns cannot contain alphabetical characters, which can occur due to scientific notation.\n",
        "\n",
        "This can be more difficult to determine in stings, in which case you may have to check using regex.\n",
        "\n",
        "- **Measured by**: Comparison between the data and metadata or documentation for the data item;\n",
        "- **Units**: Percentage of data items deemed Valid or Invalid;\n",
        "- **Related to**: Accuracy, Completeness, Consistency, and Uniqueness."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2yqaqfwBX-V"
      },
      "source": [
        "We need to first define what we expect from our dataset:\n",
        "\n",
        "- stock: string (nullable = true) => Should be contained in the list of expected tickers\n",
        "- date: date (nullable = true) => Should conform to date format, and be in the past\n",
        "- open: double (nullable = true) => Should be positive or 0\n",
        "- high: double (nullable = true) => Should be positive or 0\n",
        "- low: double (nullable = true) => Should be positive or 0 (should be < high)\n",
        "- close: double (nullable = true) => Should be positive or 0 (should be <= high )\n",
        "- adj_close: double (nullable = true) => Should be positive or 0\n",
        "- volume: integer (nullable = true) => Should be positive or 0\n",
        "- high_avg: double (nullable = true) => Derived (not needed to test)\n",
        "- high_imp: double (nullable = true) => Derived (not needed to test)\n",
        "- day_of_week: string (nullable = true) => Derived (not needed to test)\n",
        "\n",
        "*Is there any other logic that we can incorporate?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpi4u_J0BX-W"
      },
      "source": [
        "> â¹ï¸ **Instructions** â¹ï¸\n",
        ">\n",
        "> Use the metadata to check if all the stocks in your current dataframe are valid. In other words, make sure you have no foreign/unknown tickers in your dataframe.\n",
        ">\n",
        ">*You may use as many coding cells as necessary.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9FOoGJXBX-W"
      },
      "outputs": [],
      "source": [
        "#TODO: Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCuuW46GBX-W"
      },
      "source": [
        "> â¹ï¸ **Instructions** â¹ï¸\n",
        ">\n",
        ">\n",
        ">Check if the date column contains only valid dates and all dates are in the past.\n",
        ">\n",
        ">*Valid dates should already be checked in the data reading step.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcNBsmSJBX-W"
      },
      "outputs": [],
      "source": [
        "#TODO: Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc9cgIzzBX-X"
      },
      "source": [
        "> â¹ï¸ **Instructions** â¹ï¸\n",
        ">\n",
        ">Check that all of numerical columns are positive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fepfICIyBX-X"
      },
      "outputs": [],
      "source": [
        "#TODO: Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRb9s19XBX-X"
      },
      "source": [
        "### Save Updates\n",
        "\n",
        "With our initial work of checking the various dimensions of the data quality completed, we can now save these results to a CSV file.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIZiPu8KBX-X"
      },
      "outputs": [],
      "source": [
        "pd_df = df.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NFt_GulBX-X"
      },
      "outputs": [],
      "source": [
        "pd_df.to_csv('./<name>_<surname>_data_profiling.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "24a0a2ddc4dffcb168e507551dd24967ddc40ea2df7a72a200a74e0ae6d88beb"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}